{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\programdata\\anaconda3\\lib\\site-packages (1.11.0)\n",
      "Requirement already satisfied: torchvision in c:\\programdata\\anaconda3\\lib\\site-packages (0.12.0)\n",
      "Requirement already satisfied: torchaudio in c:\\programdata\\anaconda3\\lib\\site-packages (0.11.0+cu113)\n",
      "Requirement already satisfied: typing-extensions in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (4.1.1)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (2.27.1)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (1.21.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (9.0.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (2021.10.8)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "#!conda install --yes --prefix {sys.prefix} torchvision\n",
    "!{sys.executable} -m pip install --user --upgrade torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"lolol this quiz has got to be most ridiculous quiz I have ever done in my life.\n",
    "never had I just looked at the qn and select the first option that I see and click next.\n",
    "that's how much of a rush we were in lmaoo. I thought can crtl f in my pdf file. \n",
    "lmao i was so wrong. I don't even have time to think bruh\n",
    "\n",
    "\"\"\"\n",
    "doc = nlp(text)\n",
    "senlist = [sen for sen in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[,\n",
       " never,\n",
       " had,\n",
       " I,\n",
       " just,\n",
       " looked,\n",
       " at,\n",
       " the,\n",
       " qn,\n",
       " and,\n",
       " select,\n",
       " the,\n",
       " first,\n",
       " option,\n",
       " that,\n",
       " I,\n",
       " see,\n",
       " and,\n",
       " click,\n",
       " next,\n",
       " .]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = Counter(senlist[1])\n",
    "word_freq.most_common(5)\n",
    "unique_words = [word for (word, freq) in word_freq.items() if freq == 1]\n",
    "unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ridiculous', 'first', 'much', 'wrong']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_list = []\n",
    "for sen in senlist:\n",
    "    for token in sen:\n",
    "        if token.pos_ == \"ADJ\":\n",
    "            adj_list.append(token.text)\n",
    "adj_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-processing\n",
    "def is_token_allowed(token):\n",
    "    if (not token or not token.text.strip() or token.is_stop or token.is_punct):\n",
    "        return False\n",
    "    return True\n",
    "def preprocess_token(token):\n",
    "    return token.lemma_.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lolol',\n",
       " 'quiz',\n",
       " 'get',\n",
       " 'ridiculous',\n",
       " 'quiz',\n",
       " 'life',\n",
       " 'look',\n",
       " 'qn',\n",
       " 'select',\n",
       " 'option',\n",
       " 'click',\n",
       " 'rush',\n",
       " 'lmaoo',\n",
       " 'think',\n",
       " 'crtl',\n",
       " 'f',\n",
       " 'pdf',\n",
       " 'file',\n",
       " 'lmao',\n",
       " 'wrong',\n",
       " 'time',\n",
       " 'think',\n",
       " 'bruh']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_filtered_tokens = [preprocess_token(token) for token in doc if is_token_allowed(token)]\n",
    "complete_filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dog', 'KENNEL', 'dogs', 'CANINES', 'GREYHOUND', 'pet', 'Pet-Care', 'FELINE', 'cat', 'BEAGLES']\n"
     ]
    }
   ],
   "source": [
    "#Word Vectors\n",
    "your_word = \"dog\"\n",
    "\n",
    "ms = nlp.vocab.vectors.most_similar(\n",
    "    np.asarray([nlp.vocab.vectors[nlp.vocab.strings[your_word]]]), n=10) #show the first 10 most similar words to dog\n",
    "words = [nlp.vocab.strings[w] for w in ms[0][0]]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8634041115001581"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Similarity\n",
    "nlp(\"dog kennel\").similarity(nlp(\"Pooches pet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is the 100th Etext file p'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pipeline\n",
    "nlp = spacy.blank(\"en\") #creating a blank pipeline\n",
    "nlp.add_pipe(\"sentencizer\") #creating your model by adding modifiers\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "s = requests.get(\"https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt\")\n",
    "soup = BeautifulSoup(s.content).text.replace(\"-\\n\", \"\").replace(\"\\n\", \" \")[:30]\n",
    "doc = nlp(soup)\n",
    "\n",
    "#analyse pipelines\n",
    "nlp2.analyze_pipes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555 CARDINAL\n"
     ]
    }
   ],
   "source": [
    "for ent in nlp(\"(555) 555-5555\").ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rules-Based SpaCy\n",
    "#EntityRuler\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "#Create the Ruler and Add it\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", before = \"ner\")\n",
    "#List of Entities and Patterns (source: https://spacy.io/usage/rule-based-matching)\n",
    "patterns = [\n",
    "                {\"label\": \"PHONE_NUMBER\", \"pattern\": [{\"ORTH\": \"(\"}, {\"SHAPE\": \"ddd\"}, {\"ORTH\": \")\"}, {\"SHAPE\": \"ddd\"},\n",
    "                {\"ORTH\": \"-\", \"OP\": \"?\"}, {\"SHAPE\": \"dddd\"}]},\n",
    "                {\"label\": \"FILM\", \"pattern\": \"Mr. Deeds\"}\n",
    "            ]\n",
    "#add patterns to ruler\n",
    "ruler.add_patterns(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(555) 555-5555 PHONE_NUMBER\n"
     ]
    }
   ],
   "source": [
    "for ent in nlp(\"(555) 555-5555\").ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(16571425990740197027, 6, 7)] EMAIL_ADDRESS wmattingly@aol.com\n",
      "(3232560085755078826, 0, 3) King Arthur pulled\n",
      "(3232560085755078826, 11, 13) Merlin taught\n",
      "(3232560085755078826, 18, 30) 'Oh nyo!' thought Arthur 'Merlin saw me'\n"
     ]
    }
   ],
   "source": [
    "#Matcher, takes into account linguistic components\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"LIKE_EMAIL\": True}]\n",
    "matcher.add(\"EMAIL_ADDRESS\", [pattern])\n",
    "doc = nlp(\"This is an email address: wmattingly@aol.com\")\n",
    "matches = matcher(doc)\n",
    "print(matches, nlp.vocab[matches[0][0]].text, doc[matches[0][1]:matches[0][2]]) #Lexeme, start token, end token\n",
    "\n",
    "#finding all proper nouns followed by a verb\n",
    "text = \"King Arthur pulled the sword out. This is impressive. Merlin taught him to do that.\"\n",
    "    \n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"POS\": \"PROPN\", \"OP\": \"+\"}, {\"POS\": \"VERB\"}]\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern], greedy='LONGEST')\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "matches.sort(key = lambda x: x[1])\n",
    "for match in matches[:10]:\n",
    "    print (match, doc[match[1]:match[2]])\n",
    "    \n",
    "#finding all speeches and their speaker\n",
    "text = \"King Arthur pulled the sword out. This is impressive. Merlin taught him to do that. 'Oh nyo!' thought Arthur 'Merlin saw me'\"\n",
    "\n",
    "speak_lemmas = [\"think\", \"say\"]\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern1 = [{'ORTH': \"'\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': \"'\"}, {\"POS\": \"VERB\", \"LEMMA\": {\"IN\": speak_lemmas}}, {\"POS\": \"PROPN\", \"OP\": \"+\"}, {'ORTH': \"'\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': \"'\"}]\n",
    "pattern2 = [{'ORTH': \"'\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': \"'\"}, {\"POS\": \"VERB\", \"LEMMA\": {\"IN\": speak_lemmas}}, {\"POS\": \"PROPN\", \"OP\": \"+\"}]\n",
    "pattern3 = [{\"POS\": \"PROPN\", \"OP\": \"+\"},{\"POS\": \"VERB\", \"LEMMA\": {\"IN\": speak_lemmas}}, {'ORTH': \"'\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': \"'\"}]\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern1, pattern2, pattern3], greedy='LONGEST')\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "matches.sort(key = lambda x: x[1])\n",
    "for match in matches[:10]:\n",
    "    print (match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Britain GPE\n",
      "Tom PERSON\n",
      "['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer', 'remove_gpe']\n",
      "Tom PERSON\n"
     ]
    }
   ],
   "source": [
    "#Custom Components\n",
    "from spacy.language import Language\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Britain is a place. Tom is a doctor.\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "    \n",
    "@Language.component(\"remove_gpe\") #decorator\n",
    "def remove_gpe(doc):\n",
    "    original_ents = list(doc.ents)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"GPE\":\n",
    "            original_ents.remove(ent)\n",
    "    doc.ents = original_ents\n",
    "    return doc\n",
    "#print(nlp.analyze_pipes())\n",
    "\n",
    "nlp.add_pipe(\"remove_gpe\", last = True)\n",
    "print(nlp.pipe_names)\n",
    "doc = nlp(\"Britain is a place. Tom is a doctor.\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2 February', '2', 'February'), ('14 August', '4', 'August')]\n",
      "<re.Match object; span=(0, 11), match='Paul Newman'>\n",
      "<re.Match object; span=(39, 53), match='Paul Hollywood'>\n",
      "Paul Newman PERSON\n",
      "Paul Hollywood PERSON\n"
     ]
    }
   ],
   "source": [
    "#RegEx, independent of linguistic features\n",
    "import re\n",
    "pattern = r\"((\\d){1,2} (January|February|March|April|May|June|July|August|September|October|November|December))\"\n",
    "\n",
    "text = \"This is a date 2 February. Another date would be 14 August.\"\n",
    "matches = re.findall(pattern, text)\n",
    "print (matches)\n",
    "\n",
    "text = \"Paul Newman was an American actor, but Paul Hollywood is a British TV Host. The name Paul is quite common.\"\n",
    "pattern = r\"Paul [A-Z]\\w+\"\n",
    "matches = re.finditer(pattern, text)\n",
    "for match in matches:\n",
    "    print(match)\n",
    "    \n",
    "from spacy.tokens import Span\n",
    "nlp = spacy.blank(\"en\")\n",
    "doc = nlp(text)\n",
    "\n",
    "@Language.component(\"paul_ner\")\n",
    "def paul_ner(doc):\n",
    "    original_ents = list(doc.ents)\n",
    "    mwt_ents = []\n",
    "    for match in re.finditer(pattern, doc.text):\n",
    "        start, end = match.span()\n",
    "        span = doc.char_span(start, end)\n",
    "        if span is not None:\n",
    "            mwt_ents.append((span.start, span.end, span.text))\n",
    "    for ent in mwt_ents:\n",
    "        start, end, name = ent\n",
    "        per_ent = Span(doc, start, end, label=\"PERSON\")\n",
    "        original_ents.append(per_ent)\n",
    "    doc.ents = original_ents\n",
    "    return doc\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe(\"paul_ner\")\n",
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to C:\\Users\\Yan\n",
      "[nltk_data]     Rong\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\twitter_samples.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "text = twitter_samples.strings('tweets.20150430-223406.json')\n",
    "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'#': True,\n",
       "  'F': True,\n",
       "  'o': True,\n",
       "  'l': True,\n",
       "  'w': True,\n",
       "  'r': True,\n",
       "  'i': True,\n",
       "  'd': True,\n",
       "  'a': True,\n",
       "  'y': True,\n",
       "  ' ': True,\n",
       "  '@': True,\n",
       "  'n': True,\n",
       "  'c': True,\n",
       "  'e': True,\n",
       "  '_': True,\n",
       "  'I': True,\n",
       "  't': True,\n",
       "  'P': True,\n",
       "  'K': True,\n",
       "  'u': True,\n",
       "  'h': True,\n",
       "  '5': True,\n",
       "  '7': True,\n",
       "  'M': True,\n",
       "  'p': True,\n",
       "  's': True,\n",
       "  'f': True,\n",
       "  'b': True,\n",
       "  'g': True,\n",
       "  'm': True,\n",
       "  'k': True,\n",
       "  ':': True,\n",
       "  ')': True},\n",
       " 'Positive')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)\n",
    "\n",
    "positive_tokens_for_model = get_tweets_for_model(positive_tweets)\n",
    "negative_tokens_for_model = get_tweets_for_model(negative_tweets)\n",
    "positive_dataset = [(tweet_dict, \"Positive\")\n",
    "                     for tweet_dict in positive_tokens_for_model]\n",
    "positive_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "classifier = NaiveBayesClassifier.train(positive_dataset)\n",
    "\n",
    "#print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2\n",
      "Name: Score, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "##textblob\n",
    "from scrapeReddit import *\n",
    "\n",
    "# Function for sentiment analysis (TextBlob -> pre trained)\n",
    "def sentAnalysis(string):\n",
    "    blob = TextBlob(string)\n",
    "    sentiment = blob.sentiment.polarity\n",
    "    return round(sentiment,2)\n",
    "\n",
    "# Function to scrape posts and give sentiment score based on Post Text \n",
    "def postSentiment(subreddit, limitNum, analysisFunc):\n",
    "    modPosts_dict = {\"Title\": [], \"Post Text\": [], \"Score\": [], \"ID\":[], \"Post Object\":[],\n",
    "              \"Sentimental Score\": [], \"Post URL\": [], \"Comment Sentiment\": []\n",
    "              }\n",
    "    for post in subreddit.hot(limit=limitNum):\n",
    "        for word in post.title.split(\" \"):  #for each word in the post title(its type is list)\n",
    "            if filterPost(modPosts_dict, word, post.id, \"ID\"):\n",
    "                \n",
    "                sentimentPost = analysisFunc(post.selftext)\n",
    "                sentimentComment = commentSentiment(post, analysisFunc)\n",
    "\n",
    "                modPosts_dict[\"Title\"].append(post.title)\n",
    "                modPosts_dict[\"Post Text\"].append(post.selftext)\n",
    "                modPosts_dict[\"Score\"].append(post.score)\n",
    "                modPosts_dict[\"Sentimental Score\"].append(sentimentPost)\n",
    "                modPosts_dict[\"Post URL\"].append(post.url)\n",
    "                modPosts_dict[\"ID\"].append(post.id)\n",
    "                modPosts_dict[\"Post Object\"].append(post)\n",
    "                modPosts_dict[\"Comment Sentiment\"].append(sentimentComment)\n",
    "\n",
    "\n",
    "    posts = pd.DataFrame(modPosts_dict)\n",
    "    return posts\n",
    "\n",
    "def commentSentiment(post, analysisFunc):\n",
    "    commentsDF = getComments(post)\n",
    "    lst = []\n",
    "    scores = []\n",
    "    for comment in commentsDF[\"Comment Text\"]:\n",
    "        firstWord = comment.split(\" \")[0]\n",
    "        sentiment = analysisFunc(comment)\n",
    "        scores.append(sentiment)\n",
    "        scores.append(firstWord)\n",
    "        lst.append(scores)\n",
    "        scores = []\n",
    "    return lst\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #find_top_five(nus_sub)\n",
    "    #print(scrapeModPosts(nus_sub, 200))\n",
    "    print(postSentiment(nus_sub, 50, sentAnalysis)[\"Score\"])\n",
    "\n",
    "\n",
    "#problem = code crashes when limit is set high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
