{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4cc6d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "#from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2225c289",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d60f788d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"lolol this quiz has got to be most ridiculous quiz I have ever done in my life.\n",
    "never had I just looked at the qn and select the first option that I see and click next.\n",
    "that's how much of a rush we were in lmaoo. I thought can crtl f in my pdf file. \n",
    "lmao i was so wrong. I don't even have time to think bruh :)\n",
    "\"\"\"\n",
    "def preprocess(text):\n",
    "    doc = nlp(text)\n",
    "    cleaned = []\n",
    "    for token in doc:\n",
    "        if not token.is_stop and not (token.text in string.punctuation) and token.text!= \"\\n\":\n",
    "            cleaned.append(token.lemma_.lower())\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5ffff2c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lolol',\n",
       " 'quiz',\n",
       " 'get',\n",
       " 'ridiculous',\n",
       " 'quiz',\n",
       " 'life',\n",
       " 'look',\n",
       " 'qn',\n",
       " 'select',\n",
       " 'option',\n",
       " 'click',\n",
       " 'rush',\n",
       " 'lmaoo',\n",
       " 'think',\n",
       " 'crtl',\n",
       " 'f',\n",
       " 'pdf',\n",
       " 'file',\n",
       " 'lmao',\n",
       " 'wrong',\n",
       " 'time',\n",
       " 'think',\n",
       " 'bruh',\n",
       " ':)']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "aa3e0050",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [1,2,3,4,5]\n",
    "y = [9,8,7,6,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ab323d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train-test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ff042eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c766936a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spacy_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [78]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Pipelining\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer(tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mspacy_tokenizer\u001b[49m, ngram_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m      3\u001b[0m classifier \u001b[38;5;241m=\u001b[39m LinearSVC()\n\u001b[0;32m      4\u001b[0m pipe \u001b[38;5;241m=\u001b[39m Pipeline([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcleaner\u001b[39m\u001b[38;5;124m\"\u001b[39m, predictors()),\n\u001b[0;32m      5\u001b[0m                  (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvectorizer\u001b[39m\u001b[38;5;124m'\u001b[39m, vectorizer),\n\u001b[0;32m      6\u001b[0m                  (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m'\u001b[39m, classifier)])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spacy_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "#Pipelining\n",
    "vectorizer = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))\n",
    "classifier = LinearSVC()\n",
    "pipe = Pipeline([(\"cleaner\", predictors()),\n",
    "                 ('vectorizer', vectorizer),\n",
    "                 ('classifier', classifier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f866d808",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('https://raw.githubusercontent.com/MicrosoftDocs/ml-basics/master/data/daily-bike-share.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea4efabb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>mnth</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>rentals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.344167</td>\n",
       "      <td>0.363625</td>\n",
       "      <td>0.805833</td>\n",
       "      <td>0.160446</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.363478</td>\n",
       "      <td>0.353739</td>\n",
       "      <td>0.696087</td>\n",
       "      <td>0.248539</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.196364</td>\n",
       "      <td>0.189405</td>\n",
       "      <td>0.437273</td>\n",
       "      <td>0.248309</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.212122</td>\n",
       "      <td>0.590435</td>\n",
       "      <td>0.160296</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.226957</td>\n",
       "      <td>0.229270</td>\n",
       "      <td>0.436957</td>\n",
       "      <td>0.186900</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   season  mnth  holiday  weekday  workingday  weathersit      temp     atemp  \\\n",
       "0       1     1        0        6           0           2  0.344167  0.363625   \n",
       "1       1     1        0        0           0           2  0.363478  0.353739   \n",
       "2       1     1        0        1           1           1  0.196364  0.189405   \n",
       "3       1     1        0        2           1           1  0.200000  0.212122   \n",
       "4       1     1        0        3           1           1  0.226957  0.229270   \n",
       "\n",
       "        hum  windspeed  rentals  \n",
       "0  0.805833   0.160446      331  \n",
       "1  0.696087   0.248539      131  \n",
       "2  0.437273   0.248309      120  \n",
       "3  0.590435   0.160296      108  \n",
       "4  0.436957   0.186900       82  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bc6f390",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['season'\n",
    "             , 'mnth'\n",
    "             , 'holiday'\n",
    "             , 'weekday'\n",
    "             , 'workingday'\n",
    "             , 'weathersit'\n",
    "             , 'temp'\n",
    "             , 'atemp'\n",
    "             , 'hum'\n",
    "             , 'windspeed'\n",
    "             , 'rentals']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5760bd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting into train and test\n",
    "X = data.drop('rentals',axis=1)\n",
    "y = data['rentals']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a0d6eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating transformers\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "       ('imputer', SimpleImputer(strategy='mean'))\n",
    "      ,('scaler', StandardScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "       ('imputer', SimpleImputer(strategy='constant'))\n",
    "      ,('encoder', OrdinalEncoder())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fb835f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split columns into numeric/categorical\n",
    "numeric_features = ['temp', 'atemp', 'hum', 'windspeed']\n",
    "categorical_features = ['season', 'mnth', 'holiday', 'weekday', 'workingday', 'weathersit']\n",
    "preprocessor = ColumnTransformer(\n",
    "   transformers=[\n",
    "    ('numeric', numeric_transformer, numeric_features)\n",
    "   ,('categorical', categorical_transformer, categorical_features)\n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fee315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimator pipeline\n",
    "pipeline = Pipeline(steps = [\n",
    "               ('preprocessor', preprocessor)\n",
    "              ,('regressor',RandomForestRegressor())\n",
    "           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fb59d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(transformers=[('numeric',\n",
      "                                                  Pipeline(steps=[('imputer',\n",
      "                                                                   SimpleImputer()),\n",
      "                                                                  ('scaler',\n",
      "                                                                   StandardScaler())]),\n",
      "                                                  ['temp', 'atemp', 'hum',\n",
      "                                                   'windspeed']),\n",
      "                                                 ('categorical',\n",
      "                                                  Pipeline(steps=[('imputer',\n",
      "                                                                   SimpleImputer(strategy='constant')),\n",
      "                                                                  ('encoder',\n",
      "                                                                   OrdinalEncoder())]),\n",
      "                                                  ['season', 'mnth', 'holiday',\n",
      "                                                   'weekday', 'workingday',\n",
      "                                                   'weathersit'])])),\n",
      "                ('regressor', RandomForestRegressor())])\n"
     ]
    }
   ],
   "source": [
    "rf_model = pipeline.fit(X_train, y_train)\n",
    "print (rf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f75685c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7698523765881256\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "predictions = rf_model.predict(X_test)\n",
    "print (r2_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5b83163",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import joblib\n",
    "#joblib.dump(rf_model, './rf_model.pkl') #save trained model in a file\n",
    "#rf_model = joblib.load('PATH/TO/rf_model.pkl') #load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e64246a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 699.76,  576.08,  584.93,  271.53, 1004.67, 1836.12,  493.79,\n",
       "        803.84,  924.04,  307.74,  596.35, 1489.11, 2507.53, 1202.76,\n",
       "        299.96,  849.09, 1359.26, 2033.9 ,  763.5 ,  661.24, 2498.91,\n",
       "       2071.7 ,   54.2 ,  915.04,  803.45,  208.06,  775.89,   62.05,\n",
       "        745.01,  561.82, 1186.87,  710.13, 1115.68, 1204.52, 1270.6 ,\n",
       "       1042.38,  847.56,  699.7 ,  845.34,  212.69,  794.83,   98.37,\n",
       "        274.37,  592.99,  471.15,  691.47,  701.66,  799.94,  240.51,\n",
       "        138.48,  511.96,  354.52,  796.24,  343.42,  760.49,  789.3 ,\n",
       "       1290.78,  797.73,  701.63, 1192.47,  972.2 ,  170.27,  635.1 ,\n",
       "       2126.87, 1402.48,  127.39,   62.69,  478.93,  285.24, 1747.95,\n",
       "        754.81,  327.66,  950.95,  820.58,  132.39,  672.65,  139.23,\n",
       "        498.8 , 1197.83,  264.6 ,  589.13,  638.24,  714.87,  786.53,\n",
       "       1437.64,  807.07,  282.95, 1165.09, 1075.14,  208.69,  813.21,\n",
       "       2194.78,  591.07,  489.21,  389.38, 2226.97,  275.66,  728.97,\n",
       "        656.66,  591.18,  517.1 , 1166.47,  342.67, 2491.61,  495.74,\n",
       "        299.26,  289.65,  443.66,   67.86,  591.2 ,  909.32, 2083.17,\n",
       "        731.82,  851.89,  195.29,  300.68,  461.7 ,  808.68,  465.65,\n",
       "        196.05,  201.56, 1031.14, 1125.74,  970.8 , 1506.38, 2347.16,\n",
       "        690.14,  736.98,  934.41, 1227.76,  843.02,  799.96, 2110.88,\n",
       "        300.1 ,  558.26,  695.62, 1551.25,  936.15, 1743.03, 1457.65,\n",
       "        231.67, 2309.72, 2223.56,  987.96,  811.19, 1133.  ,  766.82])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "933f120c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "488     764\n",
       "421     515\n",
       "91      898\n",
       "300     456\n",
       "177     854\n",
       "       ... \n",
       "631    2454\n",
       "548     904\n",
       "439    1005\n",
       "449    1532\n",
       "124     614\n",
       "Name: rentals, Length: 147, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cced7c",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dcf0757d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Pipeline([\n",
    " ('count vectorizer',CountVectorizer(stop_words=stopwords,lowercase=True)),\n",
    " ('chi2score',SelectKBest(chi2,k=1000)),\n",
    " ('tf_transformer',TfidfVectorizer(use_idf=True))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44709386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "class TextPreprocessor():\n",
    "#    def __init__(self, variety=\"BrE\", user_abbrevs={}, n_jobs=1):\n",
    "#        self.variety = variety\n",
    "#        self.user_abbrevs = user_abbrevs\n",
    "#        self.n_jobs = n_jobs\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def transform(self, text):\n",
    "        cleaned = []\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        for message in text:\n",
    "            str = \"\"\n",
    "            doc = nlp(message)\n",
    "            for token in doc:\n",
    "                if not token.is_stop and not (token.text in string.punctuation) and token.text!= \"\\n\":\n",
    "                    str += token.lemma_.lower() + \" \"\n",
    "            cleaned.append(str[0:len(str)-1])\n",
    "        return cleaned\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66b996a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "RFR = Pipeline([\n",
    "    #(\"text_preprocess\", TextPreprocessor()),\n",
    "    ('count vectorizer', CountVectorizer()),\n",
    "    ('chi2score', SelectKBest(chi2,k=50)),\n",
    "    ('tf_transformer', TfidfTransformer()),\n",
    "    ('regressor', RandomForestRegressor())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d19f3a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Personally, it was manageable doing in the sam...</td>\n",
       "      <td>Joy</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I thought it would be super difficult with man...</td>\n",
       "      <td>Sad</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The math department's passing mark is not very...</td>\n",
       "      <td>Joy</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From what I know, it is very hard to fail, pro...</td>\n",
       "      <td>Joy</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is it just me or is this mod just insanely dif...</td>\n",
       "      <td>Anger</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>is well liked cause the prof is great, and the...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>It was fun but like someone else said, languag...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>I can easily say that it is the most useful or...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>Probably the most fun mod I've taken in NUS so...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>It was really a really fun mod going through t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Comment Emotion  Rating\n",
       "0    Personally, it was manageable doing in the sam...     Joy       8\n",
       "1    I thought it would be super difficult with man...     Sad       3\n",
       "2    The math department's passing mark is not very...     Joy       6\n",
       "3    From what I know, it is very hard to fail, pro...     Joy       6\n",
       "4    is it just me or is this mod just insanely dif...   Anger       1\n",
       "..                                                 ...     ...     ...\n",
       "145  is well liked cause the prof is great, and the...     NaN       8\n",
       "146  It was fun but like someone else said, languag...     NaN       5\n",
       "147  I can easily say that it is the most useful or...     NaN       8\n",
       "148  Probably the most fun mod I've taken in NUS so...     NaN      10\n",
       "149  It was really a really fun mod going through t...     NaN       9\n",
       "\n",
       "[150 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"Comments Review - Sheet1.csv\")[[\"Comment\",\"Emotion\",\"Rating\"]]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0725f473",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = RFR.fit(dataset[\"Comment\"], dataset[\"Rating\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "973bcc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter text: The email was very comprehensive but ultimately have no resolution whatsoever. Its all full of jargons that ultimately defends themselves.  They really didn't think anything is wrong with using SQL query to marks open ended qns about ethics and instead highlighted the inconsequential typo of Aggregate-Utility and Aggregate Utility.  And while they promised to mark leniently, they never say they r going to mark manually. Arguably that can means SQL 'like' or fuzzy search or whatever technology they can get their hands on.  Moreover they still defend the 'experiential' learning that IS1103 has. Things like matching graduation robe. Deducing where the fictional professor died. Why the fictional Rabbit AI is like that. Putting timestamps on a hoax video and things irrelevant to IS 1103 and they still have the gall to imply in the email that it is our fault by not getting used to the experiential learning because we r used to didactic learning.  The professor also still defend himself and says that he responds to all emails asking about mission progress even though everyone knows his response is just that ' ur activity is tracked' no percentage of completions no nothing.  I am very disappointed with the response. Expected too much from an internal review. Internal review is Internal Review after all.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3.83141667])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = input(\"Enter text: \")\n",
    "model1.predict([text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a0a677d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "NB = Pipeline([\n",
    "    (\"text_preprocess\", TextPreprocessor()),\n",
    "    ('count vectorizer', CountVectorizer()),\n",
    "    ('chi2score', SelectKBest(chi2,k=50)),\n",
    "    ('tf_transformer', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "934afa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = NB.fit(dataset[\"Comment\"], dataset[\"Rating\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f68df74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter text: The email was very comprehensive but ultimately have no resolution whatsoever. Its all full of jargons that ultimately defends themselves.  They really didn't think anything is wrong with using SQL query to marks open ended qns about ethics and instead highlighted the inconsequential typo of Aggregate-Utility and Aggregate Utility.  And while they promised to mark leniently, they never say they r going to mark manually. Arguably that can means SQL 'like' or fuzzy search or whatever technology they can get their hands on.  Moreover they still defend the 'experiential' learning that IS1103 has. Things like matching graduation robe. Deducing where the fictional professor died. Why the fictional Rabbit AI is like that. Putting timestamps on a hoax video and things irrelevant to IS 1103 and they still have the gall to imply in the email that it is our fault by not getting used to the experiential learning because we r used to didactic learning.  The professor also still defend himself and says that he responds to all emails asking about mission progress even though everyone knows his response is just that ' ur activity is tracked' no percentage of completions no nothing.  I am very disappointed with the response. Expected too much from an internal review. Internal review is Internal Review after all.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = input(\"Enter text: \")\n",
    "model2.predict([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "945e36b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(model1, open(\"RFR_model.sav\", 'wb'))\n",
    "pickle.dump(model2, open(\"NB_model.sav\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81bb29a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "model = pickle.load(open(\"RFR_model.sav\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f9eab09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.83141667])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import string\n",
    "model.predict([\"The email was very comprehensive but ultimately have no resolution whatsoever. Its all full of jargons that ultimately defends themselves.  They really didn't think anything is wrong with using SQL query to marks open ended qns about ethics and instead highlighted the inconsequential typo of Aggregate-Utility and Aggregate Utility.  And while they promised to mark leniently, they never say they r going to mark manually. Arguably that can means SQL 'like' or fuzzy search or whatever technology they can get their hands on.  Moreover they still defend the 'experiential' learning that IS1103 has. Things like matching graduation robe. Deducing where the fictional professor died. Why the fictional Rabbit AI is like that. Putting timestamps on a hoax video and things irrelevant to IS 1103 and they still have the gall to imply in the email that it is our fault by not getting used to the experiential learning because we r used to didactic learning.  The professor also still defend himself and says that he responds to all emails asking about mission progress even though everyone knows his response is just that ' ur activity is tracked' no percentage of completions no nothing.  I am very disappointed with the response. Expected too much from an internal review. Internal review is Internal Review after all.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b3b3edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "424906c0efac429b9cf10aa6ccecf448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/0.98k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebb7e619efb244b382fee55ec46ead12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/313M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5adfd3cc114478287eb595a2f938ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/294 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "973ffb694e29486b8354467640cb62e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/780k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c78e6a0ad7814484a75e6fde0343cce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dda1638116f94b75b44a5ba9002781a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11b6961579394d80adde0745117db581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[[{'label': 'anger', 'score': 0.004419791977852583},\n",
       "  {'label': 'disgust', 'score': 0.001611992483958602},\n",
       "  {'label': 'fear', 'score': 0.0004138525982853025},\n",
       "  {'label': 'joy', 'score': 0.9771687984466553},\n",
       "  {'label': 'neutral', 'score': 0.005764591973274946},\n",
       "  {'label': 'sadness', 'score': 0.002092391485348344},\n",
       "  {'label': 'surprise', 'score': 0.008528684265911579}]]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Emotion\n",
    "from transformers import pipeline\n",
    "classifier = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\", return_all_scores=True)\n",
    "classifier(\"I love this!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1867e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'anger', 'score': 0.7377618551254272},\n",
       "  {'label': 'disgust', 'score': 0.034230828285217285},\n",
       "  {'label': 'fear', 'score': 0.008528497070074081},\n",
       "  {'label': 'joy', 'score': 0.005797873251140118},\n",
       "  {'label': 'neutral', 'score': 0.08096954226493835},\n",
       "  {'label': 'sadness', 'score': 0.05976967513561249},\n",
       "  {'label': 'surprise', 'score': 0.07294177263975143}]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "I thought it would be super difficult with many proof questions, so i practised to prove every theorem in the textbook and finished all exercise questions. Then it turned out that 70% of the paper(qn1-3) is all about computation, which is really difficult for me as i am careless and i didn't spend much time on practising calculating. So i spent most of my time working on qn1-3, which made me had insufficient time to finish qn4, but i still got qn2 and 3 wrong.\n",
    "\"\"\"\n",
    "classifier(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b600753",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Yan\n",
      "[nltk_data]     Rong\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Yan\n",
      "[nltk_data]     Rong\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Yan\n",
      "[nltk_data]     Rong\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Happy': 0.33, 'Angry': 0.0, 'Surprise': 0.33, 'Sad': 0.17, 'Fear': 0.17}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import text2emotion as te\n",
    "#import nltk\n",
    "#nltk.download('omw-1.4')\n",
    "text = \"For reference, I'm taking CS1010E with friends and if you make the effort to practice and think through your assignments, you'll do fine.\" \n",
    "te.get_emotion(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b4bceeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "RFR = Pipeline([\n",
    "    ('count vectorizer', CountVectorizer()),\n",
    "    ('chi2score', SelectKBest(chi2,k=100)),\n",
    "    ('tf_transformer', TfidfTransformer()),\n",
    "    ('regressor', RandomForestRegressor())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c05b4eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment1 = \"The email was very comprehensive but ultimately have no resolution whatsoever.\"\n",
    "comment2 = \"They really didn't think anything is wrong with using SQL query to marks open ended qns about ethics and instead highlighted the inconsequential typo of Aggregate-Utility and Aggregate Utility.\"\n",
    "comments = []\n",
    "comments.append(comment1)\n",
    "comments.append(comment2)\n",
    "comments\n",
    "def text_preprocess(comments):\n",
    "    cleaned = []\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    for message in comments:\n",
    "        str = \"\"\n",
    "        doc = nlp(message)\n",
    "        for token in doc:\n",
    "            if not token.is_stop and not (token.text in string.punctuation) and token.text!= \"\\n\":\n",
    "                str += token.lemma_.lower() + \" \"\n",
    "        cleaned.append(str[0:len(str)-1])\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9c342aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['think wrong sql query mark open end qns ethic instead highlight inconsequential typo aggregate utility aggregate utility']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import string\n",
    "text_preprocess([comment2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a0540d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = RFR.fit(text_preprocess(dataset[\"Comment\"]), dataset[\"Rating\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "383595fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model1, open(\"RFR_model.sav\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b36325a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def scrape_n_posts(mod, n):\n",
    "    PATH = \"C:/Users/Yan Rong/Documents/programming/Orbital/testsite/chromedriver.exe\"\n",
    "    option = Options()\n",
    "    option.add_argument(\"--disable-infobars\")\n",
    "    option.add_argument(\"start-maximized\")\n",
    "    option.add_argument(\"--disable-extensions\")\n",
    "\n",
    "    option.add_experimental_option(\n",
    "    \"prefs\", {\"profile.default_content_setting_values.notifications\": 1}\n",
    ")\n",
    "    driver = webdriver.Chrome(options = option, executable_path = PATH)\n",
    "    driver.get(\"https://www.reddit.com/r/nus/\")\n",
    "    search = driver.find_element_by_id(\"header-search-bar\")\n",
    "    search.send_keys(mod)\n",
    "    time.sleep(3)\n",
    "    search.send_keys(Keys.RETURN)\n",
    "\n",
    "    result = []\n",
    "    scale = 0\n",
    "    inturl = driver.current_url\n",
    "    while scale < n:\n",
    "        try:\n",
    "            main = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, \"//div[@class='QBfRw7Rj8UkxybFpX-USO']\")))\n",
    "        except:\n",
    "            driver.close()\n",
    "            return [\"Error\"]\n",
    "        links = main.find_elements(By.XPATH,f'//a[contains(@href,\"{mod.lower()}\")]')\n",
    "        links[scale].click()\n",
    "        #driver.switch_to.window(driver.window_handles[1])\n",
    "        url = driver.current_url\n",
    "        driver.get(url)\n",
    "        html =  BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        comments = html.find_all(\"p\",{\"class\":\"_1qeIAgB0cPwnLhDF9XSiJM\"})\n",
    "        for comment in comments:\n",
    "            result.append(comment.text)\n",
    "        #driver.close()\n",
    "        #driver.switch_to.window(driver.window_handles[0])\n",
    "        scale += 1\n",
    "        driver.get(inturl)\n",
    "    driver.close()\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e6b9c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEH1074 = scrape_n_posts(\"GEH1074\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a95e2b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Want to clear my GE mods, deciding between this or GEC1018/GEH1054 Names as Markers of Socio-cultural Identity. Both seems interesting to me.',\n",
       " \"Any seniors taken any of the above mods before? How's the prof like? How was the workload?\",\n",
       " 'Took GEH1074 luck last year. Highly recommend it, the topics taught were very interesting and the weekly workload can be quite light (varies for person to person)',\n",
       " 'Topics taught were things like how being born in earlier months is considered luckier than those born in later months, along with showing evidence and research to prove it. Or how luck and genetics are related, how being born in the right country, race, gender is considered lucky, whether we have free will etc.',\n",
       " 'There’s 1 lecture per week, and usually there’s really short quizzes during then, the no right or wrong kind just anyhow press also can, and this contributes to like 20-30% of your grades I think? I can’t really remember anymore besides it was a good portion just for attending the lectures, so highly recommend to just do so.',\n",
       " 'There’s also tutorials, 1 hour, you don’t even have to do the tutorial questions beforehand, just attend, they’ll randomly group you with others to do 1 question and then someone will present the answers. The answers you give are again not graded, the tutorials are just for you to better understand the content and get some participation marks which is like 10-20%? Really easy to get it too since no one is actively fighting for it and everyone gets a chance.',\n",
       " 'There’s also weekly quizzes, all the answers can be easily obtained from the weekly readings or is mentioned during the lectures.',\n",
       " 'Exam is just 5-6 essay questions, but I think as long as you get the point across you’ll get the mark. I didn’t really write my answers in a very formal writing style like we do for English compo just a here’s my points and my reasons on why I think this way kind.',\n",
       " 'The lecturer is also very friendly and nice. His explanations are very clear. I recommend you try take notes during his lectures or during tutorials, cause he likes to elaborate on things more in depth. Taking notes helped save a lot of time for me during revision.',\n",
       " 'The most amount of work for this module is probably the weekly readings. Some weeks are like 10 pages, some weeks can go up to 40+.   Usually the tutorial questions are related to the readings. But to be honest, you can forgo the readings throughout the semester and just take a day or two (depending on fast you read) to spam and read through everything before the exam. That’s what I did for some weeks and for the quiz I just scan through the text to find the answers. The exam do test stuff from the readings so beware.',\n",
       " 'All in all it was a pretty interesting mod. If you’re interested in the content and don’t mind reading go for it. If reading is the bane of your existence avoid it like the plague cause quite a few people I know who hated this mod was mostly due to the readings and not the content. This was honestly one of my fav modules I took and workload wise was one of the least.',\n",
       " \"Thx for your detailed reply! I've seen other reviews on the names mod and decided to go for this one\",\n",
       " 'I am considering these two GEHs and am looking for reviews for them:',\n",
       " 'GEH1034 - Clean Energy and Storage',\n",
       " 'GEH1074 - Luck',\n",
       " 'And I welcome any other GEH recommendations for this sad engineering student here 🥴Cheers fam!',\n",
       " \"hey! took geh1074 last sem and it was a fun ge mod. this module is not about probability or statistics so don't be mislead by the title.\",\n",
       " 'this module talks about the concept of luck in our universe and society. some examples of the topic the prof covered included luck and the cosmos, luck and free will, luck and circumstances at birth, luck and the marketplace etc.',\n",
       " 'i dont know how much has changed because of covid but grading when i took it last year was based on online quizzes, individual paper, tutorial participation/attendance, lecture attendance and finals.',\n",
       " \"workload wise it was quite manageable imo. you have a few readings to do weekly so you can participate in tutorial discussions but you should be able to finish them within 2-3 hours. you can choose your own topic for the individual paper and you're supposed to write an essay about your chosen topic with at least a few credible citations(the prof will cover how to find and do proper academic references during tutorials). the finals was an e-exam and if you finish the readings and understand them you should be able to do fine.\",\n",
       " \"bellcurve is quite steep because it isn't a super tough mod. try to participate during tutorials because the prof is quite friendly and encourages participation. lmk if you need more info\",\n",
       " 'Hi, thanks for the review! Are you able to send me the materials so that I can have a look before deciding to take the mod or not?',\n",
       " 'Thank you for the review, you should put your review on nusmods because it is really helpful to know !',\n",
       " \"I took GEH1034 two sems ago and I'm not sure if I'll recommend it ahahhshs. I think it should be really simple and straightforward if you're an Engineering student. (I'm a math student whose physics knowledge was limited to O levels)\",\n",
       " \"Nonetheless, i find the module q interesting because you'll get to explore various energy systems and evaluate which ones are good and bad.\",\n",
       " 'Assessment wise, there were digital midterms and final quizzes, done during class time (albeit my midterms was done after school hours). There were weekly quizzes too, which were fairly simple because you can check your notes (although the quiz uses examplify). A good understanding of basic physics will make the module a breeze. Lastly, there was also a term essay to be written with a partner, where yguys will find and discuss case-studies of energy production and storage systems.',\n",
       " 'Tbh i find the mod a bit troublesome + lecturer was a bit... intense 🙃🙃  but ngl i did learn q a lot, the fortnightly lab sessions were kinda fun.',\n",
       " \"Take it if you want to dabble a bit into Clean Energy and Storage systems. But do standby an S/U, just in case you can't score :')\",\n",
       " 'Thanks for the review! Honestly, it seems like it could complement my major (Chem Eng), but i’m afraid of the workload since I’ll be doing 4 crazy core mods in this upcoming sem, together with another engin writing mod. How’s the workload like? Were there loads of readings and how were tutorials like?',\n",
       " 'Hi! engine freshie considering this mod too. what do you mean by the mod is a little troublesome? is its workload much heavier than other geh mods? Also, I saw some of the notes on google and they say the course uses textbooks. Did you actually use them? Where did you get them?']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GEH1074"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a67e60f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model1.predict(text_preprocess(GEH1074))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90cc03ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Want to clear my GE mods, deciding between this or GEC1018/GEH1054 Names as Markers of Socio-cultural Identity. Both seems interesting to me. 4.258333333333334\n",
      "Any seniors taken any of the above mods before? How's the prof like? How was the workload? 4.1466666666666665\n",
      "Took GEH1074 luck last year. Highly recommend it, the topics taught were very interesting and the weekly workload can be quite light (varies for person to person) 5.382107680717975\n",
      "Topics taught were things like how being born in earlier months is considered luckier than those born in later months, along with showing evidence and research to prove it. Or how luck and genetics are related, how being born in the right country, race, gender is considered lucky, whether we have free will etc. 5.065055573511456\n",
      "There’s 1 lecture per week, and usually there’s really short quizzes during then, the no right or wrong kind just anyhow press also can, and this contributes to like 20-30% of your grades I think? I can’t really remember anymore besides it was a good portion just for attending the lectures, so highly recommend to just do so. 5.877558108558108\n",
      "There’s also tutorials, 1 hour, you don’t even have to do the tutorial questions beforehand, just attend, they’ll randomly group you with others to do 1 question and then someone will present the answers. The answers you give are again not graded, the tutorials are just for you to better understand the content and get some participation marks which is like 10-20%? Really easy to get it too since no one is actively fighting for it and everyone gets a chance. 4.416882114617409\n",
      "There’s also weekly quizzes, all the answers can be easily obtained from the weekly readings or is mentioned during the lectures. 6.750000000000001\n",
      "Exam is just 5-6 essay questions, but I think as long as you get the point across you’ll get the mark. I didn’t really write my answers in a very formal writing style like we do for English compo just a here’s my points and my reasons on why I think this way kind. 3.6151108474858473\n",
      "The lecturer is also very friendly and nice. His explanations are very clear. I recommend you try take notes during his lectures or during tutorials, cause he likes to elaborate on things more in depth. Taking notes helped save a lot of time for me during revision. 5.1579999999999995\n",
      "The most amount of work for this module is probably the weekly readings. Some weeks are like 10 pages, some weeks can go up to 40+.   Usually the tutorial questions are related to the readings. But to be honest, you can forgo the readings throughout the semester and just take a day or two (depending on fast you read) to spam and read through everything before the exam. That’s what I did for some weeks and for the quiz I just scan through the text to find the answers. The exam do test stuff from the readings so beware. 6.9175\n",
      "All in all it was a pretty interesting mod. If you’re interested in the content and don’t mind reading go for it. If reading is the bane of your existence avoid it like the plague cause quite a few people I know who hated this mod was mostly due to the readings and not the content. This was honestly one of my fav modules I took and workload wise was one of the least. 7.158333333333335\n",
      "Thx for your detailed reply! I've seen other reviews on the names mod and decided to go for this one 4.649142857142859\n",
      "I am considering these two GEHs and am looking for reviews for them: 5.015514662624955\n",
      "GEH1034 - Clean Energy and Storage 5.015514662624955\n",
      "GEH1074 - Luck 5.382107680717975\n",
      "And I welcome any other GEH recommendations for this sad engineering student here 🥴Cheers fam! 5.015514662624955\n",
      "hey! took geh1074 last sem and it was a fun ge mod. this module is not about probability or statistics so don't be mislead by the title. 5.33\n",
      "this module talks about the concept of luck in our universe and society. some examples of the topic the prof covered included luck and the cosmos, luck and free will, luck and circumstances at birth, luck and the marketplace etc. 5.219050703381587\n",
      "i dont know how much has changed because of covid but grading when i took it last year was based on online quizzes, individual paper, tutorial participation/attendance, lecture attendance and finals. 5.59472693972694\n",
      "workload wise it was quite manageable imo. you have a few readings to do weekly so you can participate in tutorial discussions but you should be able to finish them within 2-3 hours. you can choose your own topic for the individual paper and you're supposed to write an essay about your chosen topic with at least a few credible citations(the prof will cover how to find and do proper academic references during tutorials). the finals was an e-exam and if you finish the readings and understand them you should be able to do fine. 6.852535353535354\n",
      "bellcurve is quite steep because it isn't a super tough mod. try to participate during tutorials because the prof is quite friendly and encourages participation. lmk if you need more info 4.649142857142859\n",
      "Hi, thanks for the review! Are you able to send me the materials so that I can have a look before deciding to take the mod or not? 4.649142857142859\n",
      "Thank you for the review, you should put your review on nusmods because it is really helpful to know ! 3.2225\n",
      "I took GEH1034 two sems ago and I'm not sure if I'll recommend it ahahhshs. I think it should be really simple and straightforward if you're an Engineering student. (I'm a math student whose physics knowledge was limited to O levels) 5.37878354978355\n",
      "Nonetheless, i find the module q interesting because you'll get to explore various energy systems and evaluate which ones are good and bad. 5.015514662624955\n",
      "Assessment wise, there were digital midterms and final quizzes, done during class time (albeit my midterms was done after school hours). There were weekly quizzes too, which were fairly simple because you can check your notes (although the quiz uses examplify). A good understanding of basic physics will make the module a breeze. Lastly, there was also a term essay to be written with a partner, where yguys will find and discuss case-studies of energy production and storage systems. 6.595574043603456\n",
      "Tbh i find the mod a bit troublesome + lecturer was a bit... intense 🙃🙃  but ngl i did learn q a lot, the fortnightly lab sessions were kinda fun. 4.7892337662337665\n",
      "Take it if you want to dabble a bit into Clean Energy and Storage systems. But do standby an S/U, just in case you can't score :') 4.728052364302364\n",
      "Thanks for the review! Honestly, it seems like it could complement my major (Chem Eng), but i’m afraid of the workload since I’ll be doing 4 crazy core mods in this upcoming sem, together with another engin writing mod. How’s the workload like? Were there loads of readings and how were tutorials like? 6.597424242424243\n",
      "Hi! engine freshie considering this mod too. what do you mean by the mod is a little troublesome? is its workload much heavier than other geh mods? Also, I saw some of the notes on google and they say the course uses textbooks. Did you actually use them? Where did you get them? 4.649142857142859\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(GEH1074)):\n",
    "    print(GEH1074[i], results[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba287525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.2364722127001535"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(results)/len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9fbf2baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Want to clear my GE mods, deciding between this or GEC1018/GEH1054 Names as Markers of Socio-cultural Identity. Both seems interesting to me. {'Happy': 0.25, 'Angry': 0.0, 'Surprise': 0.0, 'Sad': 0.5, 'Fear': 0.25}\n",
      "Any seniors taken any of the above mods before? How's the prof like? How was the workload? {'Happy': 0.0, 'Angry': 0.0, 'Surprise': 1.0, 'Sad': 0.0, 'Fear': 0.0}\n",
      "Took GEH1074 luck last year. Highly recommend it, the topics taught were very interesting and the weekly workload can be quite light (varies for person to person) {'Happy': 0.5, 'Angry': 0.0, 'Surprise': 0.25, 'Sad': 0.25, 'Fear': 0.0}\n",
      "Topics taught were things like how being born in earlier months is considered luckier than those born in later months, along with showing evidence and research to prove it. Or how luck and genetics are related, how being born in the right country, race, gender is considered lucky, whether we have free will etc. {'Happy': 0.2, 'Angry': 0.0, 'Surprise': 0.07, 'Sad': 0.4, 'Fear': 0.33}\n",
      "There’s 1 lecture per week, and usually there’s really short quizzes during then, the no right or wrong kind just anyhow press also can, and this contributes to like 20-30% of your grades I think? I can’t really remember anymore besides it was a good portion just for attending the lectures, so highly recommend to just do so. {'Happy': 0.25, 'Angry': 0.12, 'Surprise': 0.38, 'Sad': 0.0, 'Fear': 0.25}\n",
      "There’s also tutorials, 1 hour, you don’t even have to do the tutorial questions beforehand, just attend, they’ll randomly group you with others to do 1 question and then someone will present the answers. The answers you give are again not graded, the tutorials are just for you to better understand the content and get some participation marks which is like 10-20%? Really easy to get it too since no one is actively fighting for it and everyone gets a chance. {'Happy': 0.0, 'Angry': 0.0, 'Surprise': 0.33, 'Sad': 0.11, 'Fear': 0.56}\n",
      "There’s also weekly quizzes, all the answers can be easily obtained from the weekly readings or is mentioned during the lectures. {'Happy': 0.0, 'Angry': 0.0, 'Surprise': 0.0, 'Sad': 0.0, 'Fear': 1.0}\n",
      "Exam is just 5-6 essay questions, but I think as long as you get the point across you’ll get the mark. I didn’t really write my answers in a very formal writing style like we do for English compo just a here’s my points and my reasons on why I think this way kind. {'Happy': 0.11, 'Angry': 0.0, 'Surprise': 0.56, 'Sad': 0.11, 'Fear': 0.22}\n",
      "The lecturer is also very friendly and nice. His explanations are very clear. I recommend you try take notes during his lectures or during tutorials, cause he likes to elaborate on things more in depth. Taking notes helped save a lot of time for me during revision. {'Happy': 0.22, 'Angry': 0.0, 'Surprise': 0.33, 'Sad': 0.22, 'Fear': 0.22}\n",
      "The most amount of work for this module is probably the weekly readings. Some weeks are like 10 pages, some weeks can go up to 40+.   Usually the tutorial questions are related to the readings. But to be honest, you can forgo the readings throughout the semester and just take a day or two (depending on fast you read) to spam and read through everything before the exam. That’s what I did for some weeks and for the quiz I just scan through the text to find the answers. The exam do test stuff from the readings so beware. {'Happy': 0.17, 'Angry': 0.17, 'Surprise': 0.0, 'Sad': 0.17, 'Fear': 0.5}\n",
      "All in all it was a pretty interesting mod. If you’re interested in the content and don’t mind reading go for it. If reading is the bane of your existence avoid it like the plague cause quite a few people I know who hated this mod was mostly due to the readings and not the content. This was honestly one of my fav modules I took and workload wise was one of the least. {'Happy': 0.3, 'Angry': 0.0, 'Surprise': 0.2, 'Sad': 0.1, 'Fear': 0.4}\n",
      "Thx for your detailed reply! I've seen other reviews on the names mod and decided to go for this one {'Happy': 0.0, 'Angry': 0.0, 'Surprise': 0.0, 'Sad': 0.0, 'Fear': 1.0}\n",
      "I am considering these two GEHs and am looking for reviews for them: {'Happy': 0.0, 'Angry': 0.0, 'Surprise': 0.0, 'Sad': 1.0, 'Fear': 0.0}\n",
      "GEH1034 - Clean Energy and Storage {'Happy': 0, 'Angry': 0, 'Surprise': 0, 'Sad': 0, 'Fear': 0}\n",
      "GEH1074 - Luck {'Happy': 1.0, 'Angry': 0.0, 'Surprise': 0.0, 'Sad': 0.0, 'Fear': 0.0}\n",
      "And I welcome any other GEH recommendations for this sad engineering student here 🥴Cheers fam! {'Happy': 0.33, 'Angry': 0.0, 'Surprise': 0.0, 'Sad': 0.33, 'Fear': 0.33}\n",
      "hey! took geh1074 last sem and it was a fun ge mod. this module is not about probability or statistics so don't be mislead by the title. {'Happy': 0.2, 'Angry': 0.0, 'Surprise': 0.4, 'Sad': 0.0, 'Fear': 0.4}\n",
      "this module talks about the concept of luck in our universe and society. some examples of the topic the prof covered included luck and the cosmos, luck and free will, luck and circumstances at birth, luck and the marketplace etc. {'Happy': 0.75, 'Angry': 0.0, 'Surprise': 0.0, 'Sad': 0.0, 'Fear': 0.25}\n",
      "i dont know how much has changed because of covid but grading when i took it last year was based on online quizzes, individual paper, tutorial participation/attendance, lecture attendance and finals. {'Happy': 0.0, 'Angry': 0.0, 'Surprise': 0.33, 'Sad': 0.33, 'Fear': 0.33}\n",
      "workload wise it was quite manageable imo. you have a few readings to do weekly so you can participate in tutorial discussions but you should be able to finish them within 2-3 hours. you can choose your own topic for the individual paper and you're supposed to write an essay about your chosen topic with at least a few credible citations(the prof will cover how to find and do proper academic references during tutorials). the finals was an e-exam and if you finish the readings and understand them you should be able to do fine. {'Happy': 0.33, 'Angry': 0.0, 'Surprise': 0.0, 'Sad': 0.0, 'Fear': 0.67}\n",
      "bellcurve is quite steep because it isn't a super tough mod. try to participate during tutorials because the prof is quite friendly and encourages participation. lmk if you need more info {'Happy': 0.5, 'Angry': 0.0, 'Surprise': 0.0, 'Sad': 0.0, 'Fear': 0.5}\n",
      "Hi, thanks for the review! Are you able to send me the materials so that I can have a look before deciding to take the mod or not? {'Happy': 0.0, 'Angry': 0.0, 'Surprise': 0.0, 'Sad': 0.0, 'Fear': 1.0}\n",
      "Thank you for the review, you should put your review on nusmods because it is really helpful to know ! {'Happy': 0.33, 'Angry': 0.0, 'Surprise': 0.33, 'Sad': 0.33, 'Fear': 0.0}\n",
      "I took GEH1034 two sems ago and I'm not sure if I'll recommend it ahahhshs. I think it should be really simple and straightforward if you're an Engineering student. (I'm a math student whose physics knowledge was limited to O levels) {'Happy': 0.0, 'Angry': 0.0, 'Surprise': 0.57, 'Sad': 0.0, 'Fear': 0.43}\n",
      "Nonetheless, i find the module q interesting because you'll get to explore various energy systems and evaluate which ones are good and bad. {'Happy': 0.33, 'Angry': 0.0, 'Surprise': 0.0, 'Sad': 0.33, 'Fear': 0.33}\n",
      "Assessment wise, there were digital midterms and final quizzes, done during class time (albeit my midterms was done after school hours). There were weekly quizzes too, which were fairly simple because you can check your notes (although the quiz uses examplify). A good understanding of basic physics will make the module a breeze. Lastly, there was also a term essay to be written with a partner, where yguys will find and discuss case-studies of energy production and storage systems. {'Happy': 0.12, 'Angry': 0.0, 'Surprise': 0.12, 'Sad': 0.12, 'Fear': 0.62}\n",
      "Tbh i find the mod a bit troublesome + lecturer was a bit... intense 🙃🙃  but ngl i did learn q a lot, the fortnightly lab sessions were kinda fun. {'Happy': 0.33, 'Angry': 0.0, 'Surprise': 0.0, 'Sad': 0.33, 'Fear': 0.33}\n",
      "Take it if you want to dabble a bit into Clean Energy and Storage systems. But do standby an S/U, just in case you can't score :') {'Happy': 0.0, 'Angry': 0.5, 'Surprise': 0.0, 'Sad': 0.0, 'Fear': 0.5}\n",
      "Thanks for the review! Honestly, it seems like it could complement my major (Chem Eng), but i’m afraid of the workload since I’ll be doing 4 crazy core mods in this upcoming sem, together with another engin writing mod. How’s the workload like? Were there loads of readings and how were tutorials like? {'Happy': 0.0, 'Angry': 0.0, 'Surprise': 0.0, 'Sad': 0.2, 'Fear': 0.8}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi! engine freshie considering this mod too. what do you mean by the mod is a little troublesome? is its workload much heavier than other geh mods? Also, I saw some of the notes on google and they say the course uses textbooks. Did you actually use them? Where did you get them? {'Happy': 0.0, 'Angry': 0.0, 'Surprise': 0.5, 'Sad': 0.25, 'Fear': 0.25}\n"
     ]
    }
   ],
   "source": [
    "for comment in GEH1074:\n",
    "    print(comment, te.get_emotion(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64ff2b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Happy': 6.220000000000001,\n",
       "         'Sad': 5.080000000000001,\n",
       "         'Fear': 11.47,\n",
       "         'Surprise': 5.370000000000001,\n",
       "         'Angry': 0.79})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "emotions_dict = {\"Happy\": 0.0, \"Angry\" : 0.0, \"Surprise\" : 0.0, \"Sad\" : 0.0, \"Fear\" : 0.0}\n",
    "for comment in GEH1074:\n",
    "    emotions_dict = Counter(emotions_dict) + Counter(te.get_emotion(comment))\n",
    "emotions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b0a93dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.930000000000003"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(list(emotions_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "35ce2f23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.929999999999996"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = 0\n",
    "for comment in GEH1074:\n",
    "    e_dict = te.get_emotion(comment)\n",
    "    total = sum(list(e_dict.values()))\n",
    "    result += total\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "36ce970c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0,2.0,3.0,4.0,5.0'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions = \"1,2,3,4,5\"\n",
    "emotions = emotions.split(\",\")\n",
    "result = list(map(float, emotions))\n",
    "end = \"\"\n",
    "for s in list(map(str, result)):\n",
    "    end += s + \",\"\n",
    "end[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ab202681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1.0', '2.0', '3.0', '4.0', '5.0', '']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'1.0,2.0,3.0,4.0,5.0,'.split(\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613f92e5",
   "metadata": {},
   "source": [
    "# ?, thank you, less than 7 words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
